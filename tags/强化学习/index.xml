<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>强化学习 on DW的个人博客</title>
    <link>https://Dumbledore696.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 强化学习 on DW的个人博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jul 2021 17:16:19 +0800</lastBuildDate><atom:link href="https://Dumbledore696.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DRL书学习笔记</title>
      <link>https://Dumbledore696.github.io/post/drl%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 13 Jul 2021 17:16:19 +0800</pubDate>
      
      <guid>https://Dumbledore696.github.io/post/drl%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>笔记 王树森DRL
  蒙特卡洛是一大类随机算法的总称，通过随机样本来估计真实值
  状态是对当前环境的一个概括，是做决策的唯一依据
  环境用于生成新状态
  动作价值函数是回报的条件期望，而求期望是为了消除回报的随机性
  策略评估是求某一个状态和动作的价值函数，动作控制是利用最佳价值函数选择某一状态下的动作
  在value-based中，求最佳策略的前提是求最优价值函数，而最优价值函数是用表格法或者网络近似的。
  时序差分中，用目标Q值或者称为TD目标代替Gt进行loss的计算，目标是缩小TD误差。比如Q-learning中的TD目标为 $$ \underbrace{Q\left(s_{t}, a_{t} ; \boldsymbol{w}\right)}_{\text {预测 } \hat{q}_{t}} \approx \underbrace{r_{t}+\gamma \cdot \max _{a \in \mathcal{A}} Q\left(s_{t+1}, a ; \boldsymbol{w}\right)}_{\mathrm{TD} \text { 目标 } \hat{y}_{t}} . $$
  RL中困难的数学推导主要是随机变量、马尔可夫决策过程、贝尔曼方程（概率计算，期望）
  off-policy和on-policy
 策略分为行为策略和目标策略，行为策略是和环境交互的策略 off-policy即行为策略和目标策略不同，因此可以使用行为策略产生的经验用于训练目标策略 经验回访只适用于off-policy，因此可得到DDPG中的DPG也是off-policy 可以看算法中A‘的生成，若A’是由行为策略生成的（即产生A的策略），则该算法为on-policy，反之为off-policy    Q-learning与SARSA的区别
   Q-learning 近似$Q_*$（与$\pi$无关） off-policy 可以使用经验回放     SARSA 近似$Q_\pi$ on-policy 不可以使用经验回放      MC和TD</description>
    </item>
    
    <item>
      <title>运行论文中算法gym出现问题OSError: [WinError 126] 找不到指定的模块 (ale_c.dll)</title>
      <link>https://Dumbledore696.github.io/post/rl_problem1/</link>
      <pubDate>Thu, 17 Jun 2021 16:40:19 +0800</pubDate>
      
      <guid>https://Dumbledore696.github.io/post/rl_problem1/</guid>
      <description>记录一下SB问题 刚开始遇到这个问题，我怀疑是没有atari中没有这个dll文件，找了半天怎么找都找不到，然后看到这是gym[atari]模块自带的问题，和算法无关，直接删除🤣，然后报错了。原来gym环境用到了这个模块，又用pip install gym[atari]装上，依然报这个错误。
解决方法 删除atari-py ：pip uninstall atari-py
然后安装另外一个版本的atari：pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py
总结 当一个包出问题时，直接卸载，下载新包，但是注意下载方式，可能需要从github中下载</description>
    </item>
    
  </channel>
</rss>
