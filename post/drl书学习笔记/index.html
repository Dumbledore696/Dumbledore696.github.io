<!doctype html>
<html lang="en-us">
  <head>
    <title>DRL书学习笔记 // DW的个人博客</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.83.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="DW" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://Dumbledore696.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="DRL书学习笔记"/>
<meta name="twitter:description" content="笔记 王树森DRL
  蒙特卡洛是一大类随机算法的总称，通过随机样本来估计真实值
  状态是对当前环境的一个概括，是做决策的唯一依据
  环境用于生成新状态
  动作价值函数是回报的条件期望，而求期望是为了消除回报的随机性
  策略评估是求某一个状态和动作的价值函数，动作控制是利用最佳价值函数选择某一状态下的动作
  在value-based中，求最佳策略的前提是求最优价值函数，而最优价值函数是用表格法或者网络近似的。
  时序差分中，用目标Q值或者称为TD目标代替Gt进行loss的计算，目标是缩小TD误差。比如Q-learning中的TD目标为 $$ \underbrace{Q\left(s_{t}, a_{t} ; \boldsymbol{w}\right)}_{\text {预测 } \hat{q}_{t}} \approx \underbrace{r_{t}&#43;\gamma \cdot \max _{a \in \mathcal{A}} Q\left(s_{t&#43;1}, a ; \boldsymbol{w}\right)}_{\mathrm{TD} \text { 目标 } \hat{y}_{t}} . $$
  RL中困难的数学推导主要是随机变量、马尔可夫决策过程、贝尔曼方程（概率计算，期望）
  off-policy和on-policy
 策略分为行为策略和目标策略，行为策略是和环境交互的策略 off-policy即行为策略和目标策略不同，因此可以使用行为策略产生的经验用于训练目标策略 经验回访只适用于off-policy，因此可得到DDPG中的DPG也是off-policy 可以看算法中A‘的生成，若A’是由行为策略生成的（即产生A的策略），则该算法为on-policy，反之为off-policy    Q-learning与SARSA的区别
   Q-learning 近似$Q_*$（与$\pi$无关） off-policy 可以使用经验回放     SARSA 近似$Q_\pi$ on-policy 不可以使用经验回放      MC和TD"/>

    <meta property="og:title" content="DRL书学习笔记" />
<meta property="og:description" content="笔记 王树森DRL
  蒙特卡洛是一大类随机算法的总称，通过随机样本来估计真实值
  状态是对当前环境的一个概括，是做决策的唯一依据
  环境用于生成新状态
  动作价值函数是回报的条件期望，而求期望是为了消除回报的随机性
  策略评估是求某一个状态和动作的价值函数，动作控制是利用最佳价值函数选择某一状态下的动作
  在value-based中，求最佳策略的前提是求最优价值函数，而最优价值函数是用表格法或者网络近似的。
  时序差分中，用目标Q值或者称为TD目标代替Gt进行loss的计算，目标是缩小TD误差。比如Q-learning中的TD目标为 $$ \underbrace{Q\left(s_{t}, a_{t} ; \boldsymbol{w}\right)}_{\text {预测 } \hat{q}_{t}} \approx \underbrace{r_{t}&#43;\gamma \cdot \max _{a \in \mathcal{A}} Q\left(s_{t&#43;1}, a ; \boldsymbol{w}\right)}_{\mathrm{TD} \text { 目标 } \hat{y}_{t}} . $$
  RL中困难的数学推导主要是随机变量、马尔可夫决策过程、贝尔曼方程（概率计算，期望）
  off-policy和on-policy
 策略分为行为策略和目标策略，行为策略是和环境交互的策略 off-policy即行为策略和目标策略不同，因此可以使用行为策略产生的经验用于训练目标策略 经验回访只适用于off-policy，因此可得到DDPG中的DPG也是off-policy 可以看算法中A‘的生成，若A’是由行为策略生成的（即产生A的策略），则该算法为on-policy，反之为off-policy    Q-learning与SARSA的区别
   Q-learning 近似$Q_*$（与$\pi$无关） off-policy 可以使用经验回放     SARSA 近似$Q_\pi$ on-policy 不可以使用经验回放      MC和TD" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Dumbledore696.github.io/post/drl%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-07-13T17:16:19&#43;08:00" />
<meta property="article:modified_time" content="2021-07-13T17:16:19&#43;08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://Dumbledore696.github.io/"><img class="app-header-avatar" src="/avatar.jpg" alt="DW" /></a>
      <h1>DW的个人博客</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">Other</a>
      </nav>
      <p>学习笔记，成长快乐</p>
      <div class="app-header-social">
        
          <a href="https://github.com/Dumbledore696" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">DRL书学习笔记</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jul 13, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://Dumbledore696.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h3 id="笔记">笔记</h3>
<p><a href="%5CDRL">王树森DRL</a></p>
<ul>
<li>
<p>蒙特卡洛是一大类随机算法的总称，通过随机样本来估计真实值</p>
</li>
<li>
<p>状态是对当前环境的一个概括，是做决策的唯一依据</p>
</li>
<li>
<p>环境用于生成新状态</p>
</li>
<li>
<p>动作价值函数是回报的条件期望，而求期望是为了消除回报的随机性</p>
</li>
<li>
<p>策略评估是求某一个状态和动作的价值函数，动作控制是利用最佳价值函数选择某一状态下的动作</p>
</li>
<li>
<p>在value-based中，求最佳策略的前提是求最优价值函数，而最优价值函数是用表格法或者网络近似的。</p>
</li>
<li>
<p>时序差分中，用目标Q值或者称为TD目标代替Gt进行loss的计算，目标是缩小TD误差。比如Q-learning中的TD目标为
$$
\underbrace{Q\left(s_{t}, a_{t} ; \boldsymbol{w}\right)}_{\text {预测 } \hat{q}_{t}} \approx \underbrace{r_{t}+\gamma \cdot \max _{a \in \mathcal{A}} Q\left(s_{t+1}, a ; \boldsymbol{w}\right)}_{\mathrm{TD} \text { 目标 } \hat{y}_{t}} .
$$</p>
</li>
<li>
<p>RL中困难的数学推导主要是随机变量、马尔可夫决策过程、贝尔曼方程（概率计算，期望）</p>
</li>
<li>
<p>off-policy和on-policy</p>
<ul>
<li>策略分为行为策略和目标策略，行为策略是和环境交互的策略</li>
<li>off-policy即行为策略和目标策略不同，因此可以使用行为策略产生的经验用于训练目标策略</li>
<li>经验回访只适用于off-policy，因此可得到DDPG中的DPG也是off-policy</li>
<li>可以看算法中A‘的生成，若A’是由行为策略生成的（即产生A的策略），则该算法为on-policy，反之为off-policy</li>
</ul>
</li>
<li>
<p>Q-learning与SARSA的区别</p>
<table>
<thead>
<tr>
<th>Q-learning</th>
<th>近似$Q_*$（与$\pi$无关）</th>
<th>off-policy</th>
<th>可以使用经验回放</th>
</tr>
</thead>
<tbody>
<tr>
<td>SARSA</td>
<td>近似$Q_\pi$</td>
<td>on-policy</td>
<td>不可以使用经验回放</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>MC和TD</p>
<ul>
<li>MC是用实际观测$u_t$回报作为目标，TD使用TD目标作为目标。</li>
<li>MC的优点是无偏性，即$u_t$是$Q_\pi(s_t,a_t)$的无偏估计，即$E(u_t)=Q_\pi(s_t,a_t)$，缺点是方差大，因为回合更新的目标$U_t$的随机性来自于t+1之后所有时间的状态和动作，随机性较大因此方差大，导致收敛慢</li>
<li>TD的优点是方差小，收敛快，但是有偏差。</li>
</ul>
</li>
<li>
<p>价值学习高级技巧</p>
<ul>
<li>经验回放
<ul>
<li>优点：打破序列的相关性，加速off-policy算法的收敛</li>
<li>优先经验回放</li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
